# 1.预备知识

## 1.1 数据操作

导入numpy

```python
from mxnet import np, npx
npx.set_np()
```

  在 MXNet 中，使用 `npx.set_np()` 是为了让 MXNet 的 NumPy API 与标准的 NumPy API 兼容。

1. **MXNet 提供了一个 NumPy 兼容的 API**，使得用户可以使用类似于 NumPy 的语法和功能来处理数据。

2. 通过 `from mxnet import np, npx`，你可以直接使用 MXNet 定义的 NumPy 函数，而不是标准 NumPy 的函数。

3. 调用 `npx.set_np()` 后，MXNet 的 NumPy API 会在后台自动处理相关的计算和张量操作。

4. 这样做的好处是你可以在深度学习中利用 MXNet 的计算图和 GPU 加速，同时使用熟悉的 NumPy 语法。

   导入pytorch

   ```python
   import torch
   ```

### 什么是张量？

   张量（Tensor）可以理解为一种多维数据结构，是深度学习框架（如 PyTorch、TensorFlow）中用来存储数据的基本单位。它是标量、向量和矩阵的推广，能表示从0维到任意高维的数据。

   以下是张量在不同维度下的理解：

   1. **0维张量**：标量，一个单一的数值，比如 `3` 或 `5.7`。
   2. **1维张量**：向量，一组有序数值，比如 `[1, 2, 3]`。
   3. **2维张量**：矩阵，比如 `[[1, 2], [3, 4]]`，这可以表示表格数据或图像的一个通道。
   4. **3维张量**：可以表示一个矩阵的序列或多通道图像，比如 `[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]`。
   5. **更高维的张量**：比如4维、5维张量，在深度学习中通常用来存储批量的多通道图像或视频帧。

   张量的主要特性就是它可以进行高效的并行计算和自动微分，因此广泛用于神经网络中来表示和操作数据。

 ### 张量是如何实现自动求导的？

   张量实现自动求导的关键在于**计算图（Computational Graph）**，这是深度学习框架（如 PyTorch 和 TensorFlow）中自动求导的核心。计算图是一种有向无环图，表示变量之间的计算关系。通过计算图，每个操作（例如加法、乘法）会被记录在图中，允许框架根据这个图进行反向传播（Backpropagation）以计算梯度。

 #### 自动求导的实现步骤

   1. **构建计算图**
      每当我们对张量执行操作时，框架会将这些操作动态地添加到计算图中，记录每一步的计算关系。例如，在 PyTorch 中，如果你在创建张量时设置 `requires_grad=True`，则所有对这个张量的操作都会自动构建成一个计算图。
   2. **前向传播（Forward Pass）**
      前向传播过程中，张量在计算图中逐步通过一系列操作，得到最终的输出。所有操作的中间结果（例如激活、乘积）会被存储，以便稍后进行反向传播。
   3. **反向传播（Backward Pass）**
      反向传播时，计算图从输出节点开始，沿着图的边向输入节点反向传递梯度。根据链式法则（Chain Rule），计算图会逐步求解每个操作的梯度，并将结果反传到输入。每个节点的梯度被计算并存储到相应的张量中。
   4. **梯度累积**
      反向传播过程中，每个变量的梯度会被累积并保存在 `.grad` 属性中，允许我们对每个张量直接访问其梯度。
   5. **释放计算图**
      默认情况下，计算图在每次反向传播后会自动释放（即清除已存储的计算步骤），以节省内存。如果希望保留计算图（如在循环中多次使用），可以指定 `retain_graph=True`。

   在 PyTorch 中，自动求导的过程可以简单地实现如下：

   ```python
   import torch
   
   # 创建张量并启用自动求导
   x = torch.tensor(2.0, requires_grad=True)
   y = x ** 2 + 3 * x + 5  # 定义函数 y = x^2 + 3x + 5
   
   # 进行反向传播，计算 dy/dx
   y.backward()
   
   # 查看 x 的梯度
   print(x.grad)  # 输出：7.0
   ```

   这里，`y.backward()` 会根据计算图来计算 `y` 相对于 `x` 的导数，并存储在 `x.grad` 中。这个过程利用链式法则，通过计算图在每一步自动计算每个操作的梯度。

 #### 总结

   张量的自动求导实现主要依赖于计算图和链式法则的结合，框架会根据计算图自动记录并追踪每一步操作，以便在反向传播时有效计算梯度。这一过程允许我们在深度学习中高效地进行优化，尤其是梯度下降等方法。

### 计算图

   计算图（Computational Graph）是一种表示计算流程的有向无环图，用于追踪变量之间的计算关系。在深度学习和自动求导中，计算图扮演着核心角色，通过它可以自动地、有效地计算梯度。这种结构对神经网络的优化尤为重要，因为它使得反向传播算法能够高效地更新模型参数。

   无环性在计算图中的意义很大，尤其是自动求导的过程中：

   - **防止无限循环**：如果计算图中存在环，反向传播时会导致无限循环，计算无法结束。无环性确保每个节点只被计算一次，不会无限循环。
   - **顺序依赖性**：无环图的结构使得计算可以有条不紊地从输入到输出沿着图的边逐步推进，这对于深度学习中的前向传播和反向传播至关重要。

 #### 计算图的结构

   在计算图中：

   - **节点（Node）**：每个节点代表一个操作（例如加法、乘法）或一个变量（输入、输出、中间结果）。
   - **边（Edge）**：每条边表示数据的流动或运算关系，通常从输入节点到输出节点，形成一个有向的计算路径。

 #### 计算图的类型

   1. **静态计算图**（Static Computational Graph）
      - 静态计算图在程序运行前一次性构建，比如在 TensorFlow 1.x 中，计算图是在运行模型之前定义的，执行时通过会话（Session）运行。
      - 优点是更高效，但修改和调试起来稍显不便，因为图是固定的。
   2. **动态图**（Dynamic Computational Graph）
      - 动态计算图在每次前向传播时动态构建，比如在 PyTorch 中，计算图是动态生成的，允许更灵活地定义和修改模型。
      - 动态图在处理循环和条件逻辑时更加直观和灵活。

 #### 计算图的构建和应用

   计算图的主要应用是在自动求导过程中。以下是计算图的几个关键步骤：

   1. **前向传播**
      在前向传播中，输入数据从计算图的起点（输入节点）经过一系列操作，最终到达输出节点。计算图会记录每一步的操作（例如加法、乘法）和中间结果，为反向传播做准备。
   2. **反向传播**
      反向传播时，计算图会沿着从输出到输入的路径反向传播梯度。根据链式法则，每一步都会根据输出的梯度计算输入的梯度。这个过程允许我们高效地计算所有参数的梯度，并用于更新模型参数。

 #### 示例

   假设我们有一个简单的函数：
$$
   y = (x_1 + x_2) \times x_3
$$
   计算图可以表示为：

   1. **输入节点**：
      $$
      x_1, x_2, x_3
      $$

   2. 操作节点：

      - **加法节点**：计算
        $$
        z = x_1 + x_2
        $$

      - **乘法节点**：计算 
        $$
        y = z \times x_3
        $$

   在计算图中，计算过程会如下展示：

   ```plaintext
   x1 -----> ( + ) -----> ( * ) -----> y
                   ↑          ↑
   x2 ------------        x3
   ```

 #### 总结

   计算图的核心在于它能表示复杂的计算过程，并通过记录操作链，使得我们可以沿着图反向传播，自动计算梯度。因此，计算图在深度学习的优化过程中起着至关重要的作用。

### 链式法则在深度学习中的应用

   在神经网络中，我们通常有一个多层复合函数。比如一个三层的神经网络模型可以表示为：
$$
   y = f_3(f_2(f_1(x)))
$$
   这里
$$
   f_1、f_2、f_3
$$
   分别是网络的每一层函数（例如激活函数、加权求和等），输出结果 y是最外层函数的值。为了优化模型，我们需要计算损失 L 对每层参数的梯度。通过链式法则，我们可以逐层计算每一层的导数。

 #### 具体示例

   假设我们有一个简单的网络模型，定义如下：

   1. $$
      u = f(x)
      $$

   2. $$
      v = g(u)
      $$

   3. $$
      w = h(v)
      $$

   最终输出为 w，我们想要计算 
$$
   \frac{dw}{dx}
$$
   根据链式法则：
$$
   \frac{dw}{dx} = \frac{dw}{dv} \cdot \frac{dv}{du} \cdot \frac{du}{dx}
$$
   通过计算每一步的导数，再相乘，就可以得到最终的结果。

### 为什么不叫数组？

   张量与数组虽然在结构上有相似之处，但“张量”这个概念比数组更广泛，并且它们在数学上、应用场景上有一些关键区别。

 #### 1. **数学背景和线性代数**  
  张量源自数学和物理中的线性代数领域，它描述了多维数据的“秩”和“维度”，并且具有严格的数学定义，包含特定的变换规则。张量不仅仅是一个多维数组，它还代表不同坐标系下的数据变化，主要用于描述物理系统中的矢量和矩阵。

 #### 2. **自动微分和优化**  
  深度学习中，张量的计算需要自动求导，以便通过梯度下降等方法进行参数优化。深度学习框架中的张量结构带有计算图，支持高效的反向传播和自动微分。这些张量与普通数组的运算逻辑不同，因为它们能记录并跟踪计算步骤。

 #### 3. **设备与计算加速支持**  
  张量可以轻松切换计算设备，例如 CPU 和 GPU。深度学习框架的张量在 GPU 上能够进行并行加速计算，而普通数组（如 NumPy 数组）默认只在 CPU 上运行。张量还可以直接利用底层硬件的优化实现高效运算。

 #### 4. **广播机制**  
  张量在运算时具有广播机制，可以自动扩展不同形状的数据并进行操作，这在处理不同形状的数据时非常方便。而数组不一定自带广播机制，需要手动调整形状。

 #### 总结
   因此，张量虽然在结构上类似于数组，但它在数学概念、优化计算、设备支持等方面的优势，使它在深度学习等领域中成为更适合的选择。

   创建一个数组（numpy）

   ```python
   x = np.arange(12)
   print（x）
   ```

   ```python
   array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
   ```

   创建一个张量（pytorch）

   ```python
   x = torch.arange(12)
   print（x）
   ```

   ```python
   tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
   ```

 在 NumPy 和 PyTorch 中，`array` 和 `tensor` 代表了不同的概念和数据结构，尽管它们在功能上有很多相似之处。

 ### NumPy 的 `array`

- **定义**：`numpy.array` 是 NumPy 库中的主要数据结构，主要用于存储和处理多维数组。
- **用途**：NumPy 主要用于科学计算和数据分析，提供了丰富的数学函数和操作。
- **数据类型**：NumPy 数组可以存储多种数据类型，包括整数、浮点数、布尔值等。

 ### PyTorch 的 `tensor`

- **定义**：`torch.tensor` 是 PyTorch 库中的主要数据结构，专门用于深度学习和机器学习中的张量运算。
- **用途**：PyTorch 主要用于构建和训练神经网络，支持自动求导和 GPU 加速。
- **数据类型**：PyTorch 张量同样可以存储多种数据类型，但它更专注于支持高效的数值计算和深度学习相关的操作。

 ### 主要区别

1. **库的目的**：
   - NumPy 主要用于科学计算和数据处理。
   - PyTorch 主要用于深度学习和神经网络的构建。
2. **数据结构名称**：
   - NumPy 使用 `array` 作为其多维数组的名称。
   - PyTorch 使用 `tensor` 来表示多维数组，更符合深度学习的术语。
3. **功能与特性**：
   - PyTorch 的张量支持 GPU 加速和自动求导，这使得它在深度学习中非常高效。
   - NumPy 提供了丰富的数学和统计函数，但不支持自动求导。

可以通过张量的`shape`属性来访问张量（沿每个轴的长度）的*形状* 。

```python
x.shape
```

（numpy）

```
(12,)
```

（pytorch）

```python
torch.Size([12])
```

  NumPy 和 PyTorch 在表示数组或张量的形状时使用了不同的形式，虽然它们传达的意思是相同的。以下是两者的主要区别：

#### NumPy 的形状表示

- **输出格式**：NumPy 使用元组（tuple）来表示数组的形状。例如，对于一维数组，`x.shape` 输出 `(12,)`。
- **含义**：这个元组表示数组的维度，其中 `12` 表示数组的长度，后面的逗号表示这是一个一维数组。

1.一维数组：

```python
import numpy as np
x_1d = np.array([1, 2, 3])
print(x_1d.shape)  # 输出: (3,)
```

2.二维数组：

```python
x_2d = np.array([[1, 2, 3], [4, 5, 6]])
print(x_2d.shape)  # 输出: (2, 3)
```

- 一维数组的形状表示为 `(n,)`，其中 `n` 是元素的数量。
- 二维数组的形状表示为 `(m, n)`，其中 `m` 是行数，`n` 是列数。

#### PyTorch 的形状表示

- **输出格式**：PyTorch 使用 `torch.Size` 类来表示张量的形状。例如，`torch.Size([12])`。
- **含义**：`torch.Size` 是一个特殊的类，用于表示张量的形状，类似于元组，但它提供了一些额外的方法和功能。
- PyTorch 的 `torch.Size` 输出并不直接显示张量的维度信息，而是仅仅列出了每个维度的大小。

### 使用pytorch时候如何判断维度

要判断 PyTorch 张量的维度，可以使用 `.dim()` 方法：

```python
import torch

x_torch = torch.arange(12).reshape(2, 6)

print(x_torch.size())  # 输出: torch.Size([2, 6])
print(x_torch.dim())   # 输出: 2，表示这是一个二维张量
```

如果只想知道张量中元素的总数，即形状的所有元素乘积，可以检查它的大小（size）。 因为这里在处理的是一个向量，所以它的`shape`与它的`size`相同。

（numpy）（pytorch）

```python
x.size #输出：12
```

要想改变一个张量的形状而不改变元素数量和元素值，可以调用`reshape`函数。 例如，可以把张量`x`从形状为（12,）的行向量转换为形状为（3,4）的矩阵。 这个新的张量包含与转换前相同的值，但是它被看成一个3行4列的矩阵。 要重点说明一下，虽然张量的形状发生了改变，但其元素值并没有变。 注意，通过改变张量的形状，张量的大小不会改变。

```python
X = x.reshape(3, 4)
print（x）
```

输出（numpy）：

```python
array([[ 0.,  1.,  2.,  3.],
       [ 4.,  5.,  6.,  7.],
       [ 8.,  9., 10., 11.]])
```

输出中的点（`.`）表示这些数字是浮点数（float）。在 NumPy 中，默认情况下，数组中的整数会被转换为浮点数以保持一致性。

输出（pytorch）：

```python
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
```

在这里没有点（`.`）是因为这些数字是整数（int），而不是浮点数（float）。PyTorch 默认使用整数类型表示整型数据。

当我们不想计算当有x行时候列数y或者有y列时行数x的值那么我们可以用（x，-1）或（-1，y）来计算示例：

```python
matrix_auto_cols = x.reshape(4, -1)#我们想要4行而不知道列数
matrix_auto = x.reshape(-1, 4)#我们只知道列数（例如4列），而不想手动计算行数
```

这都将得到与上面 x = x.reshape（3，4）一样的输出结果。

有时，我们希望使用全0、全1、其他常量，或者从特定分布中随机采样的数字来初始化矩阵。 我们可以创建一个形状为（2,3,4）的张量，其中所有元素都设置为0。代码如下：

```python
np.zeros((2, 3, 4))#numpy
array([[[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]],

       [[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]]])#输出
torch.zeros((2, 3, 4))#pytorch
tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])#输出
```

在 PyTorch 中，`torch.zeros((2, 3, 4))` 默认生成的张量是浮点型（`torch.float32`），而不是整型。果你想生成整型的张量，可以在调用时指定数据类型，例如：

```python
torch.zeros((2, 3, 4), dtype=torch.int)
```

有时我们想通过从某个特定的概率分布中随机采样来得到张量中每个元素的值。 例如，当我们构造数组来作为神经网络中的参数时，我们通常会随机初始化参数的值。 以下代码创建一个形状为（3,4）的张量。 其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样。

```python
np.random.normal(0, 1, size=(3, 4))#numpy
array([[ 2.2122064 ,  1.1630787 ,  0.7740038 ,  0.4838046 ],
       [ 1.0434403 ,  0.29956347,  1.1839255 ,  0.15302546],
       [ 1.8917114 , -1.1688148 , -1.2347414 ,  1.5580711 ]])#输出（每次结果会有所不同）
torch.randn(3, 4)#pytorch
tensor([[-0.0135,  0.0665,  0.0912,  0.3212],
        [ 1.4653,  0.1843, -1.6995, -0.3036],
        [ 1.7646,  1.0450,  0.2457, -0.7732]])#输出（每次结果会有所不同）
```

`np.random.normal(0, 1, size=(3, 4))` 中的 `normal` 是 **NumPy** 提供的一个方法，用来生成符合**正态分布**（也称为高斯分布）随机数的数组。参数解释

- **0**：第一个参数，表示正态分布的均值（μ）。在这个例子中，均值是 0。
- **1**：第二个参数，表示正态分布的标准差（σ）。这里标准差是 1，表示生成的数值在均值 0 的基础上会有一定的随机波动。
- **size=(3, 4)**：指定输出数组的形状。在这里，`size=(3, 4)` 生成一个 3 行 4 列的二维数组。

这些数字符合均值为 0、标准差为 1 的正态分布。通常用于生成测试数据或在深度学习中初始化权重。

### 正态分布

**正态分布**（Normal Distribution），也称**高斯分布**（Gaussian Distribution），是一种重要的连续概率分布，在自然界和统计学中非常常见。许多自然现象的观测值（如人的身高、考试成绩、测量误差等）都会近似呈正态分布。

正态分布的概率密度函数（PDF）呈现一个对称的**钟形曲线**，具有以下特征：

1. **均值（μ）**：正态分布的中心点，即分布的最高点，是分布的均值。均值决定了正态分布在横轴上的位置。
2. **标准差（σ）**：决定分布的“宽度”，即数据在均值周围的分散程度。标准差越小，分布越集中；标准差越大，分布越分散。

![正态分布概率密度函数](https://upload.wikimedia.org/wikipedia/commons/7/74/Normal_Distribution_PDF.svg)

正态分布的概率密度函数（PDF）为：
$$
f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{(x - \mu)^2}{2 \sigma^2} \right)
$$

 #### 正态分布的性质

正态分布有以下关键性质：

1. **对称性**：正态分布的曲线关于均值对称，均值左右的数据分布是对称的。
2. **68-95-99.7 规则**（Empirical Rule）：
   - **68%** 的数据位于均值的 **1个标准差** 范围内（即 [[μ−σ,μ+σ]）。
   - **95%** 的数据位于均值的 **2个标准差** 范围内（即 [μ−2σ,μ+2σ]）。
   - **99.7%** 的数据位于均值的 **3个标准差** 范围内（即 [μ−3σ,μ+3σ]）。

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Standard_deviation_diagram.svg/2880px-Standard_deviation_diagram.svg.png" alt="1" style="zoom:33%;" />

 #### 标准正态分布

当正态分布的**均值**为 0，**标准差**为 1 时，它被称为**标准正态分布**，用 N(0,1)表示。标准正态分布的概率密度函数为：
$$
$f(x) = \frac{1}{\sqrt{2 \pi}} \exp \left( -\frac{x^2}{2} \right)$
$$
  我们还可以通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值。 在这里，最外层的列表对应于轴0，内层的列表对应于轴1。

```python
np.array([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])#numpy
array([[2., 1., 4., 3.],
       [1., 2., 3., 4.],
       [4., 3., 2., 1.]])#输出
torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])#pytorch
tensor([[2, 1, 4, 3],
        [1, 2, 3, 4],
        [4, 3, 2, 1]])#输出
```

#### 何为轴0、轴1？

在多维数组（如 NumPy 数组）中，**轴（axis）** 指的是数组的维度。理解轴可以帮助我们操作数组的不同维度，特别是在多维数组中。通常来说：

- **轴 0**（axis 0）对应最外层的维度，沿着行方向。
- **轴 1**（axis 1）对应内层的维度，沿着列方向。

在这个数组中：

- **最外层的列表**是 `[[2,1,4,3], [1,2,3,4], [4,3,2,1]]`，即行的集合。沿着这个方向的索引就是**轴 0**。
- **内层的列表**是 `[[2,1,4,3]`、`[1,2,3,4]`、`[4,3,2,1]`，即每一行中的元素。沿着这个方向的索引就是**轴 1**。

### 具体操作中的表现

当我们在 NumPy 中指定 `axis=0` 时，操作沿着行的方向应用。例如：

```python
np.sum(array, axis=0)
```

会对每列求和，输出为 `[2+1+4=7, 6, 9,8]`。

而 `axis=1` 时，操作沿着列的方向应用：

```python
np.sum(array, axis=1)
```

会对每行求和，输出为 `[2+1+4+3=10, 10, 10]`。
